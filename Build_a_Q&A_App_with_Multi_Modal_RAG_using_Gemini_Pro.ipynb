{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMmZw7bz7iPPW9g54FSl9rT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HarshShinde0/Build-a-Q-A-App-with-Multi-Modal-RAG-using-Gemini-Pro/blob/main/Build_a_Q%26A_App_with_Multi_Modal_RAG_using_Gemini_Pro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RPTvhjb4cWQu"
      },
      "outputs": [],
      "source": [
        "!pip install -U --quiet langchain langchain_community chromadb  langchain-google-vertexai\n",
        "!pip install --quiet \"unstructured[all-docs]\" pypdf pillow pydantic lxml pillow matplotlib chromadb tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO : ENter project and location\n",
        "PROJECT_ID = \"vertex-ai-00\"\n",
        "REGION = \"us-central1\"\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "CrftKFm1cp1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import vertexai\n",
        "vertexai.init(project = PROJECT_ID , location = REGION)"
      ],
      "metadata": {
        "id": "2rqjPrGRd7h4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import zipfile\n",
        "import requests\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "data_url = \"https://storage.googleapis.com/benchmarks-artifacts/langchain-docs-benchmarking/cj.zip\"\n",
        "result = requests.get(data_url)\n",
        "filename = \"cj.zip\"\n",
        "with open(filename, \"wb\") as file:\n",
        "   file.write(result.content)\n",
        "\n",
        "with zipfile.ZipFile(filename, \"r\") as zip_ref:\n",
        "   zip_ref.extractall()"
      ],
      "metadata": {
        "id": "xIW82UWdeETC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "loader = PyPDFLoader(\"./cj/cj.pdf\")\n",
        "docs = loader.load()\n",
        "tables = []\n",
        "texts = [d.page_content for d in docs]"
      ],
      "metadata": {
        "id": "Ib-r84fWeHBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts[0]"
      ],
      "metadata": {
        "id": "HWcbNsuFeLC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(texts)"
      ],
      "metadata": {
        "id": "LQ9jvOj0eNS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_vertexai import VertexAI , ChatVertexAI , VertexAIEmbeddings\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_core.messages import AIMessage\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnableLambda"
      ],
      "metadata": {
        "id": "itqvLei3eT31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate summaries of text elements\n",
        "def generate_text_summaries(texts, tables, summarize_texts=False):\n",
        "   \"\"\"\n",
        "   Summarize text elements\n",
        "   texts: List of str\n",
        "   tables: List of str\n",
        "   summarize_texts: Bool to summarize texts\n",
        "   \"\"\"\n",
        "\n",
        "   # Prompt\n",
        "   prompt_text = \"\"\"You are an assistant tasked with summarizing tables and text for retrieval. \\\n",
        "   These summaries will be embedded and used to retrieve the raw text or table elements. \\\n",
        "   Give a concise summary of the table or text that is well optimized for retrieval. Table or text: {element} \"\"\"\n",
        "   prompt = PromptTemplate.from_template(prompt_text)\n",
        "   empty_response = RunnableLambda(\n",
        "       lambda x: AIMessage(content=\"Error processing document\")\n",
        "   )\n",
        "   # Text summary chain\n",
        "   model = VertexAI(\n",
        "       temperature=0, model_name=\"gemini-pro\", max_output_tokens=1024\n",
        "   ).with_fallbacks([empty_response])\n",
        "   summarize_chain = {\"element\": lambda x: x} | prompt | model | StrOutputParser()\n",
        "\n",
        "   # Initialize empty summaries\n",
        "   text_summaries = []\n",
        "   table_summaries = []\n",
        "\n",
        "   # Apply to text if texts are provided and summarization is requested\n",
        "   if texts and summarize_texts:\n",
        "       text_summaries = summarize_chain.batch(texts, {\"max_concurrency\": 1})\n",
        "   elif texts:\n",
        "       text_summaries = texts\n",
        "\n",
        "   # Apply to tables if tables are provided\n",
        "   if tables:\n",
        "       table_summaries = summarize_chain.batch(tables, {\"max_concurrency\": 1})\n",
        "\n",
        "   return text_summaries, table_summaries\n",
        "\n",
        "\n",
        "# Get text summaries\n",
        "text_summaries, table_summaries = generate_text_summaries(\n",
        "   texts, tables, summarize_texts=True\n",
        ")\n",
        "\n",
        "text_summaries[0]"
      ],
      "metadata": {
        "id": "CR03-9U1eWcN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import base64\n",
        "import os\n",
        "\n",
        "from langchain_core.messages import HumanMessage"
      ],
      "metadata": {
        "id": "c0owMe9DeXZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_image(image_path):\n",
        "   \"\"\"Getting the base64 string\"\"\"\n",
        "   with open(image_path, \"rb\") as image_file:\n",
        "       return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
        "\n",
        "\n",
        "def image_summarize(img_base64, prompt):\n",
        "   \"\"\"Make image summary\"\"\"\n",
        "   model = ChatVertexAI(model_name=\"gemini-pro-vision\", max_output_tokens=1024)\n",
        "\n",
        "   msg = model(\n",
        "       [\n",
        "           HumanMessage(\n",
        "               content=[\n",
        "                   {\"type\": \"text\", \"text\": prompt},\n",
        "                   {\n",
        "                       \"type\": \"image_url\",\n",
        "                       \"image_url\": {\"url\": f\"data:image/jpeg;base64,{img_base64}\"},\n",
        "                   },\n",
        "               ]\n",
        "           )\n",
        "       ]\n",
        "   )\n",
        "   return msg.content\n",
        "\n",
        "\n",
        "def generate_img_summaries(path):\n",
        "   \"\"\"\n",
        "   Generate summaries and base64 encoded strings for images\n",
        "   path: Path to list of .jpg files extracted by Unstructured\n",
        "   \"\"\"\n",
        "\n",
        "   # Store base64 encoded images\n",
        "   img_base64_list = []\n",
        "\n",
        "   # Store image summaries\n",
        "   image_summaries = []\n",
        "\n",
        "   # Prompt\n",
        "   prompt = \"\"\"You are an assistant tasked with summarizing images for retrieval. \\\n",
        "   These summaries will be embedded and used to retrieve the raw image. \\\n",
        "   Give a concise summary of the image that is well optimized for retrieval.\"\"\"\n",
        "\n",
        "   # Apply to images\n",
        "   for img_file in sorted(os.listdir(path)):\n",
        "       if img_file.endswith(\".jpg\"):\n",
        "           img_path = os.path.join(path, img_file)\n",
        "           base64_image = encode_image(img_path)\n",
        "           img_base64_list.append(base64_image)\n",
        "           image_summaries.append(image_summarize(base64_image, prompt))\n",
        "\n",
        "   return img_base64_list, image_summaries\n",
        "\n",
        "\n",
        "# Image summaries\n",
        "img_base64_list, image_summaries = generate_img_summaries(\"./cj\")\n",
        "\n",
        "len(img_base64_list)\n",
        "\n",
        "len(image_summaries)\n",
        "\n",
        "image_summaries[0]"
      ],
      "metadata": {
        "id": "JfvG4PjaeZ1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import uuid\n",
        "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
        "from langchain.storage import InMemoryStore\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_core.documents import Document"
      ],
      "metadata": {
        "id": "1tm0PpWNhm9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_multi_vector_retriever(\n",
        "   vectorstore, text_summaries, texts, table_summaries, tables, image_summaries, images\n",
        "):\n",
        "   \"\"\"\n",
        "   Create retriever that indexes summaries, but returns raw images or texts\n",
        "   \"\"\"\n",
        "\n",
        "   # Initialize the storage layer\n",
        "   store = InMemoryStore()\n",
        "   id_key = \"doc_id\"\n",
        "\n",
        "   # Create the multi-vector retriever\n",
        "   retriever = MultiVectorRetriever(\n",
        "       vectorstore=vectorstore,\n",
        "       docstore=store,\n",
        "       id_key=id_key,\n",
        "   )\n",
        "\n",
        "   # Helper function to add documents to the vectorstore and docstore\n",
        "   def add_documents(retriever, doc_summaries, doc_contents):\n",
        "       doc_ids = [str(uuid.uuid4()) for _ in doc_contents]\n",
        "       summary_docs = [\n",
        "           Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
        "           for i, s in enumerate(doc_summaries)\n",
        "       ]\n",
        "       retriever.vectorstore.add_documents(summary_docs)\n",
        "       retriever.docstore.mset(list(zip(doc_ids, doc_contents)))\n",
        "\n",
        "   # Add texts, tables, and images\n",
        "   # Check that text_summaries is not empty before adding\n",
        "   if text_summaries:\n",
        "       add_documents(retriever, text_summaries, texts)\n",
        "   # Check that table_summaries is not empty before adding\n",
        "   if table_summaries:\n",
        "       add_documents(retriever, table_summaries, tables)\n",
        "   # Check that image_summaries is not empty before adding\n",
        "   if image_summaries:\n",
        "       add_documents(retriever, image_summaries, images)\n",
        "\n",
        "   return retriever\n",
        "\n",
        "\n",
        "# The vectorstore to use to index the summaries\n",
        "vectorstore = Chroma(\n",
        "   collection_name=\"mm_rag_cj_blog\",\n",
        "   embedding_function=VertexAIEmbeddings(model_name=\"textembedding-gecko@latest\"),\n",
        ")\n",
        "\n",
        "# Create retriever\n",
        "retriever_multi_vector_img = create_multi_vector_retriever(\n",
        "   vectorstore,\n",
        "   text_summaries,\n",
        "   texts,\n",
        "   table_summaries,\n",
        "   tables,\n",
        "   image_summaries,\n",
        "   img_base64_list,\n",
        ")\n"
      ],
      "metadata": {
        "id": "kUDRHkPphn2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "import re\n",
        "\n",
        "from IPython.display import HTML, display\n",
        "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "def plt_img_base64(img_base64):\n",
        "   \"\"\"Disply base64 encoded string as image\"\"\"\n",
        "   # Create an HTML img tag with the base64 string as the source\n",
        "   image_html = f'<img src=\"data:image/jpeg;base64,{img_base64}\" />'\n",
        "   # Display the image by rendering the HTML\n",
        "   display(HTML(image_html))\n",
        "\n",
        "\n",
        "def looks_like_base64(sb):\n",
        "   \"\"\"Check if the string looks like base64\"\"\"\n",
        "   return re.match(\"^[A-Za-z0-9+/]+[=]{0,2}$\", sb) is not None\n",
        "\n",
        "\n",
        "def is_image_data(b64data):\n",
        "   \"\"\"\n",
        "   Check if the base64 data is an image by looking at the start of the data\n",
        "   \"\"\"\n",
        "   image_signatures = {\n",
        "       b\"\\xFF\\xD8\\xFF\": \"jpg\",\n",
        "       b\"\\x89\\x50\\x4E\\x47\\x0D\\x0A\\x1A\\x0A\": \"png\",\n",
        "       b\"\\x47\\x49\\x46\\x38\": \"gif\",\n",
        "       b\"\\x52\\x49\\x46\\x46\": \"webp\",\n",
        "   }\n",
        "   try:\n",
        "       header = base64.b64decode(b64data)[:8]  # Decode and get the first 8 bytes\n",
        "       for sig, format in image_signatures.items():\n",
        "           if header.startswith(sig):\n",
        "               return True\n",
        "       return False\n",
        "   except Exception:\n",
        "       return False\n",
        "\n",
        "\n",
        "def resize_base64_image(base64_string, size=(128, 128)):\n",
        "   \"\"\"\n",
        "   Resize an image encoded as a Base64 string\n",
        "   \"\"\"\n",
        "   # Decode the Base64 string\n",
        "   img_data = base64.b64decode(base64_string)\n",
        "   img = Image.open(io.BytesIO(img_data))\n",
        "\n",
        "   # Resize the image\n",
        "   resized_img = img.resize(size, Image.LANCZOS)\n",
        "\n",
        "   # Save the resized image to a bytes buffer\n",
        "   buffered = io.BytesIO()\n",
        "   resized_img.save(buffered, format=img.format)\n",
        "\n",
        "   # Encode the resized image to Base64\n",
        "   return base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
        "\n",
        "\n",
        "def split_image_text_types(docs):\n",
        "   \"\"\"\n",
        "   Split base64-encoded images and texts\n",
        "   \"\"\"\n",
        "   b64_images = []\n",
        "   texts = []\n",
        "   for doc in docs:\n",
        "       # Check if the document is of type Document and extract page_content if so\n",
        "       if isinstance(doc, Document):\n",
        "           doc = doc.page_content\n",
        "       if looks_like_base64(doc) and is_image_data(doc):\n",
        "           doc = resize_base64_image(doc, size=(1300, 600))\n",
        "           b64_images.append(doc)\n",
        "       else:\n",
        "           texts.append(doc)\n",
        "   if len(b64_images) > 0:\n",
        "       return {\"images\": b64_images[:1], \"texts\": []}\n",
        "   return {\"images\": b64_images, \"texts\": texts}"
      ],
      "metadata": {
        "id": "AI-yalM7hp82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def img_prompt_func(data_dict):\n",
        "   \"\"\"\n",
        "   Join the context into a single string\n",
        "   \"\"\"\n",
        "   formatted_texts = \"\\n\".join(data_dict[\"context\"][\"texts\"])\n",
        "   messages = []\n",
        "\n",
        "   # Adding the text for analysis\n",
        "   text_message = {\n",
        "       \"type\": \"text\",\n",
        "       \"text\": (\n",
        "           \"You are financial analyst tasking with providing investment advice.\\n\"\n",
        "           \"You will be given a mixed of text, tables, and image(s) usually of charts or graphs.\\n\"\n",
        "           \"Use this information to provide investment advice related to the user question. \\n\"\n",
        "           f\"User-provided question: {data_dict['question']}\\n\\n\"\n",
        "           \"Text and / or tables:\\n\"\n",
        "           f\"{formatted_texts}\"\n",
        "       ),\n",
        "   }\n",
        "   messages.append(text_message)\n",
        "   # Adding image(s) to the messages if present\n",
        "   if data_dict[\"context\"][\"images\"]:\n",
        "       for image in data_dict[\"context\"][\"images\"]:\n",
        "           image_message = {\n",
        "               \"type\": \"image_url\",\n",
        "               \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image}\"},\n",
        "           }\n",
        "           messages.append(image_message)\n",
        "   return [HumanMessage(content=messages)]\n"
      ],
      "metadata": {
        "id": "7OXbvnFihsyz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def multi_modal_rag_chain(retriever):\n",
        "   \"\"\"\n",
        "   Multi-modal RAG chain\n",
        "   \"\"\"\n",
        "\n",
        "   # Multi-modal LLM\n",
        "   model = ChatVertexAI(\n",
        "       temperature=0, model_name=\"gemini-pro-vision\", max_output_tokens=1024\n",
        "   )\n",
        "\n",
        "   # RAG pipeline\n",
        "   chain = (\n",
        "       {\n",
        "           \"context\": retriever | RunnableLambda(split_image_text_types),\n",
        "           \"question\": RunnablePassthrough(),\n",
        "       }\n",
        "       | RunnableLambda(img_prompt_func)\n",
        "       | model\n",
        "       | StrOutputParser()\n",
        "   )\n",
        "\n",
        "   return chain\n",
        "\n",
        "\n",
        "# Create RAG chain\n",
        "chain_multimodal_rag = multi_modal_rag_chain(retriever_multi_vector_img)"
      ],
      "metadata": {
        "id": "VH-bAdDYhulh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What are the EV / NTM and NTM rev growth for MongoDB, Cloudflare, and Datadog?\"\n",
        "docs = retriever_multi_vector_img.get_relevant_documents(query, limit=1)\n",
        "\n",
        "# We get relevant docs\n",
        "len(docs)\n",
        "\n",
        "docs"
      ],
      "metadata": {
        "id": "ykuhM7zhhwqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt_img_base64(docs[0])"
      ],
      "metadata": {
        "id": "Jv5zlAcqhz0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = chain_multimodal_rag.invoke(query)\n",
        "\n",
        "from IPython.display import Markdown as md\n",
        "md(result)"
      ],
      "metadata": {
        "id": "FhQltA3Nh2kk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}